(talk about the script a bit)

OVERVIEW: The script scrapes the first 20 pages of hackernews for information about the current articles listed.

First, the browser is initiated via ```get_driver() ```:

```python
def get_driver():
    driver = webdriver.Chrome()
    return driver
```

While ```page_number``` variable is less than or equal to 20:

Step 1. Attempts to connect to hackernews via ```connect_to_base(browser, page_number)``` using the ```browser``` instance and ```page_number```:

```python
def connect_to_base(browser, page_number):
    base_url = 'https://news.ycombinator.com/news?p={0}'.format(page_number)
    try:
        browser.get(base_url)
        return True
    except:
        print('Error connecting to {0}'.format(base_url))
        return False
```

Step 2. Get the page html:

```python
html = browser.page_source
```

Step 3. Using beautifulsoup, ```output_list``` is generated by calling ```parse_html(html)```:

```python
def parse_html(html):
    # create soup object
    soup = BeautifulSoup(html, 'html.parser')
    output_list = []
    try:
        # parses soup object to get article id, rank, score, and title
        tr_blocks = soup.find_all('tr', class_='athing')
        for tr in tr_blocks:
            tr_id = tr.get('id')
            
            try:
                score = soup.find(id='score_{0}'.format(tr_id)).string
            except:
                score = '0 points'
            
            article_info = {
                'id': tr_id,
                'rank': tr.span.string,
                'score': score,
                'title': tr.find(class_='storylink').string
            }
            # append article_info to output_list
            output_list.append(article_info)
    except:
        print('parsing error')
    # returns output_list
    return output_list
```
  
Step 4. Writes ```output_list``` to ```filename```:

```python
def write_to_file(output_list, filename):
    for row in output_list:
        with open(filename, 'a') as csvfile:
            fieldnames = ['id', 'rank', 'score', 'title']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writerow(row)
```

Step 5. Increments the ```page_number``` variable by one before returning to step 1:

```python
page_number = page_number + 1
```

(dive into how you'd go about testing it w/o actually hitting the page)

To test the parsing functionality without making the making repeated the get requests, you can download the page html and pass it in as the html to be parsed by the parse_html function and then set a flag in the command line to notify the script to only parse the html

# Set Up Multiprocessing

### Prepare the ```__main__``` function for Multiprocessing:

Move the call to ```get_driver()``` inside the while loop and add ```browser.quit()``` to each instance:

```python

...

if __name__ == '__main__':
    start_time = time()
    page_number = 1
    output_timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
    filename = 'output_{0}.csv'.format(output_timestamp)
    while page_number <= 20:
        browser = get_driver()
        if connect_to_base(browser, page_number):
            sleep(2)
            html = browser.page_source
            output_list = parse_html(html)
            write_to_file(output_list, filename) 
            page_number = page_number + 1
            browser.quit()
        else:
            print('Error connecting to Hacker News')
            browser.quit()

...

```

Abstract functions out of ```__main___``` by creating ```run_process(page_number)```:

```python

...

def run_process(page_number, filename):
    browser = get_driver()
    if connect_to_base(browser, page_number):
        sleep(2)
        html = browser.page_source
        output_list = parse_html(html)
        write_to_file(output_list, filename) 
        browser.quit()
    else:
        print('Error connecting to Hacker News')
        browser.quit()


if __name__ == '__main__':
    start_time = time()
    page_number = 1
    output_timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
    filename = 'output_{0}.csv'.format(output_timestamp)
    while page_number <= 20:
        run_process(page_number, filename)
        page_number = page_number + 1

...

```

Add ```Pool, cpu_count``` modules from ```multiprocessing``` package and ```repeat``` module from ```itertools``` package to imports at top of script:

```python

...

from time import sleep, time
from itertools import repeat

from selenium import webdriver
from bs4 import BeautifulSoup
from multiprocessing import Pool, cpu_count

...

```

Refactor ```__main__``` to use ```Pool()``` in place of ```while``` loop and remove ```page_number``` variable:

```python

...

if __name__ == '__main__':
    start_time = time()
    output_timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
    filename = 'output_{0}.csv'.format(output_timestamp)
    with Pool(cpu_count()-1) as p:
        p.starmap(run_process, zip(range(1, 21), repeat(filename)))
    p.close()
    p.join()

...

```

Go headless:

```python

...

def get_driver():
    # initialize options
    options = webdriver.ChromeOptions()
    # pass in headless argument to options
    options.add_argument('--headless')
    # initialize driver
    driver = webdriver.Chrome(chrome_options=options)
    return driver

...

```
