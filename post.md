(talk about the script a bit)

OVERVIEW: The script scrapes the first 20 pages of hackernews for information about the current articles listed.

First, the browser is initiated via ```get_driver() ```:

```python
def get_driver():
    driver = webdriver.Chrome()
    return driver
```

While ```page_number``` variable is less than or equal to 20:

Step 1. Attempts to connect to hackernews via ```connect_to_base(browser, page_number)``` using the ```browser``` instance and ```page_number```:

```python
def connect_to_base(browser, page_number):
    base_url = 'https://news.ycombinator.com/news?p={0}'.format(page_number)
    try:
        browser.get(base_url)
        return True
    except:
        print('Error connecting to {0}'.format(base_url))
        return False
```

Step 2. Get the page html:

```python
html = browser.page_source
```

Step 3. Using beautifulsoup, ```output_list``` is generated by calling ```parse_html(html)```:

```python
def parse_html(html):
    # create soup object
    soup = BeautifulSoup(html, 'html.parser')
    output_list = []
    try:
        # parses soup object to get article id, rank, score, and title
        tr_blocks = soup.find_all('tr', class_='athing')
        for tr in tr_blocks:
            tr_id = tr.get('id')
            
            try:
                score = soup.find(id='score_{0}'.format(tr_id)).string
            except:
                score = '0 points'
            
            article_info = {
                'id': tr_id,
                'rank': tr.span.string,
                'score': score,
                'title': tr.find(class_='storylink').string
            }
            # append article_info to output_list
            output_list.append(article_info)
    except:
        print('parsing error')
    # returns output_list
    return output_list
```
  
Step 4. Writes ```output_list``` to ```filename```:

```python
def write_to_file(output_list, filename):
    for row in output_list:
        with open(filename, 'a') as csvfile:
            fieldnames = ['id', 'rank', 'score', 'title']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writerow(row)
```

Step 5. Increments the ```page_number``` variable by one before returning to step 1:

```python
page_number = page_number + 1
```


# Setup Basic Testing

To test the parsing functionality without initiating the driver and making the repeated get requests to the hacker news url, you can download the page html and parse it locally. 

Create a ```test``` directory:

```sh
$ mkdir test
```

Download the page html manually to the ```test``` directory and rename it ```test.html```:

![alt text](/assets/screenshot.png "Save Screenshot")

Add ```sys``` package to imports near top of file:

```python

import datetime
import csv
import sys
from time import sleep, time

```

Add logic in ```__main__``` to check for ```--test``` flag:

```python

...

if __name__ == '__main__':
    start_time = time()
    output_timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
    filename = 'output_{0}.csv'.format(output_timestamp)
    if sys.argv[1] == '--test':
        html = open('test/test.html')
        output_list = parse_html(html)
        write_to_file(output_list, filename) 
    else:
        browser = get_driver()
        page_number = 1
        while page_number <= 20:
            if connect_to_base(browser, page_number):
                sleep(2)
                html = browser.page_source
                output_list = parse_html(html)
                write_to_file(output_list, filename) 
                page_number = page_number + 1
                
            else:
                print('Error connecting to Hacker News')
        browser.quit()

...

```


# Setup Multiprocessing

In order to setup Multiprocessing we will go through the following steps. At the end we will add a flag to run Chrome headless to increase processing speed.

Move the call to ```get_driver()``` inside the while loop and add ```browser.quit()``` to each instance:

```python

...

if __name__ == '__main__':
    start_time = time()
    output_timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
    filename = 'output_{0}.csv'.format(output_timestamp)
    page_number = 1
    while page_number <= 20:
        browser = get_driver()
        if connect_to_base(browser, page_number):
            sleep(2)
            html = browser.page_source
            output_list = parse_html(html)
            write_to_file(output_list, filename) 
            page_number = page_number + 1
            browser.quit()
        else:
            print('Error connecting to Hacker News')
            browser.quit()

...

```

Abstract functions out of ```__main___``` by creating ```run_process(page_number)```:

```python

...

def run_process(page_number, filename):
    browser = get_driver()
    if connect_to_base(browser, page_number):
        sleep(2)
        html = browser.page_source
        output_list = parse_html(html)
        write_to_file(output_list, filename) 
        browser.quit()
    else:
        print('Error connecting to Hacker News')
        browser.quit()


if __name__ == '__main__':
    start_time = time()
    output_timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
    filename = 'output_{0}.csv'.format(output_timestamp)
    page_number = 1
    while page_number <= 20:
        run_process(page_number, filename)
        page_number = page_number + 1

...

```

Add ```Pool, cpu_count``` modules from ```multiprocessing``` package and ```repeat``` module from ```itertools``` package to imports at top of script:

```python

...

from time import sleep, time
from itertools import repeat

from selenium import webdriver
from bs4 import BeautifulSoup
from multiprocessing import Pool, cpu_count

...

```

Refactor ```__main__``` to use ```Pool()``` in place of ```while``` loop and remove ```page_number``` variable:

```python

...

if __name__ == '__main__':
    start_time = time()
    output_timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')
    filename = 'output_{0}.csv'.format(output_timestamp)
    with Pool(cpu_count()-1) as p:
        p.starmap(run_process, zip(range(1, 21), repeat(filename)))
    p.close()
    p.join()

...

```

We can go headless with Chrome to speed up processing by adding ```ChromeOptions()``` to the driver:

```python

...

def get_driver():
    # initialize options
    options = webdriver.ChromeOptions()
    # pass in headless argument to options
    options.add_argument('--headless')
    # initialize driver
    driver = webdriver.Chrome(chrome_options=options)
    return driver

...

```
