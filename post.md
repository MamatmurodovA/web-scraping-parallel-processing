(talk about the script a bit)

OVERVIEW: The script scrapes the first 20 pages of hackernews for information about the current articles listed.

First, the browser is initiated via ```get_driver() ```:

```python
def get_driver():
    driver = webdriver.Chrome()
    return driver
```

While page_count variable is less than or equal to 20:

Step 1. browser attempts to connect to hackernews via ```connect_to_base(browser, page_count)``` using the browser object and  page_count variable to define the url:

```python
def connect_to_base(browser, page_count):
    base_url = 'https://news.ycombinator.com/news?p={0}'.format(page_count)
    try:
        browser.get(base_url)
        return True
    except:
        print('Error connecting to {0}'.format(base_url))
        return False
```

Step 2. Get the html by calling ```browser.page_source``` on the browser instance:

```python
html = browser.page_source
```

Step 3. Using beautifulsoup ```output_list``` is generated by calling ```parse_html(html)``` on the html:

```python
def parse_html(html):
    # creates soup object
    soup = BeautifulSoup(html, 'html.parser')
    output_list = []
    try:
        # parses soup object to get article id, rank, score, and title
        tr_blocks = soup.find_all('tr', class_='athing')
        for tr in tr_blocks:
            tr_id = tr.get('id')
            a_elements = tr.find_all('a')
            
            try:
                score = soup.find(id='score_{0}'.format(tr_id)).string
            except:
                score = 'null'
            
            article_info = {
                'id': tr_id,
                'rank': tr.span.string,
                'score': score,
                'title': a_elements[1].string
            }
            # appends article_info to output_list
            output_list.append(article_info)
    except:
        print('parsing error')
    # returns output_list
    return output_list
```
  
Step 4. Writes ```output_list``` to ```filename```:

```python
def write_to_file(output_list, filename):
    for row in output_list:
        with open(filename, 'a') as csvfile:
            fieldnames = ['id', 'rank', 'score', 'title']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writerow(row)
```

Step 5. Increments the ```page_count``` variable by one before returning to step 1:
```python
page_count = page_count + 1
```

(dive into how you'd go about testing it w/o actually hitting the page)

To test the parsing functionality without making the making repeated the get requests, you can download the page html and pass it in as the html to be parsed by the parse_html function and then set a flag in the command line to notify the script to only parse the html