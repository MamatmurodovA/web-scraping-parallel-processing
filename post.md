(talk about the script a bit)

OVERVIEW: The script scrapes the first 20 pages of hackernews for information about the current articles listed.

First, the browser is initiated via ```get_driver() ```:

```python
def get_driver():
    driver = webdriver.Chrome()
    return driver
```

While ```page_count``` variable is less than or equal to 20:

Step 1. Attempts to connect to hackernews via ```connect_to_base(browser, page_count)``` using the ```browser``` instance and ```page_count```:

```python
def connect_to_base(browser, page_count):
    base_url = 'https://news.ycombinator.com/news?p={0}'.format(page_count)
    try:
        browser.get(base_url)
        return True
    except:
        print('Error connecting to {0}'.format(base_url))
        return False
```

Step 2. Get the page html:

```python
html = browser.page_source
```

Step 3. Using beautifulsoup, ```output_list``` is generated by calling ```parse_html(html)```:

```python
def parse_html(html):
    # create soup object
    soup = BeautifulSoup(html, 'html.parser')
    output_list = []
    try:
        # parses soup object to get article id, rank, score, and title
        tr_blocks = soup.find_all('tr', class_='athing')
        for tr in tr_blocks:
            tr_id = tr.get('id')
            
            try:
                score = soup.find(id='score_{0}'.format(tr_id)).string
            except:
                score = '0 points'
            
            article_info = {
                'id': tr_id,
                'rank': tr.span.string,
                'score': score,
                'title': tr.find(class_='storylink').string
            }
            # append article_info to output_list
            output_list.append(article_info)
    except:
        print('parsing error')
    # returns output_list
    return output_list
```
  
Step 4. Writes ```output_list``` to ```filename```:

```python
def write_to_file(output_list, filename):
    for row in output_list:
        with open(filename, 'a') as csvfile:
            fieldnames = ['id', 'rank', 'score', 'title']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writerow(row)
```

Step 5. Increments the ```page_count``` variable by one before returning to step 1:

```python
page_count = page_count + 1
```

(dive into how you'd go about testing it w/o actually hitting the page)

To test the parsing functionality without making the making repeated the get requests, you can download the page html and pass it in as the html to be parsed by the parse_html function and then set a flag in the command line to notify the script to only parse the html